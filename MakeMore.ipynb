{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9IBS3vUAV5jlpS53gxHdc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VinitVpANDEY/Neural-Network/blob/main/MakeMore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "url8kt8aM50u",
        "outputId": "e14e9152-34cb-47c2-8581-b9aedc0831ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['emma', 'olivia', 'ava', 'isabella', 'sophia']\n",
            "32033\n",
            "2\n",
            "15\n"
          ]
        }
      ],
      "source": [
        "words = open('names.txt', 'r').read().splitlines()\n",
        "print(words[:5])\n",
        "print(len(words))\n",
        "print(min(len(w) for w in words))\n",
        "print(max(len(w) for w in words))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for w in words[:3]:\n",
        "  chs = ['<S>'] + list(w) + ['<E>']\n",
        "  for ch1,ch2 in zip(chs, chs[1:]):\n",
        "    print(ch1, ch2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2PU9zeqfk2C",
        "outputId": "efbb8979-c7fa-43ad-fc8d-a6c64f33309a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<S> e\n",
            "e m\n",
            "m m\n",
            "m a\n",
            "a <E>\n",
            "<S> o\n",
            "o l\n",
            "l i\n",
            "i v\n",
            "v i\n",
            "i a\n",
            "a <E>\n",
            "<S> a\n",
            "a v\n",
            "v a\n",
            "a <E>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = {}\n",
        "for w in words:\n",
        "  chs = ['<S>'] + list(w) + ['<E>']\n",
        "  for ch1,ch2 in zip(chs, chs[1:]):\n",
        "    bigram = (ch1, ch2)\n",
        "    b[bigram] = b.get(bigram, 0) + 1\n",
        "\n",
        "sorted_b = sorted(b.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for k, v in sorted_b[:5]:\n",
        "    print(k, v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TqC5yGFf_gR",
        "outputId": "c2e69172-297b-4931-930d-4845716f2e22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('n', '<E>') 6763\n",
            "('a', '<E>') 6640\n",
            "('a', 'n') 5438\n",
            "('<S>', 'a') 4410\n",
            "('e', '<E>') 3983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "N = torch.zeros((27,27), dtype=torch.int32)\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0        # we are going to replace special token <S> and <E> with .\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2 in zip(chs, chs[1:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    N[ix1][ix2] += 1"
      ],
      "metadata": {
        "id": "dniY24uihOJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P = (N+1).float()\n",
        "P = P / P.sum(1, keepdim=True)\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "for i in range(10):\n",
        "  out = []\n",
        "  ix = 0\n",
        "  while True:\n",
        "    p = P[ix]    # accessing the probability density of ix'th row\n",
        "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
        "    out.append(itos[ix])\n",
        "    if ix == 0:\n",
        "      break\n",
        "\n",
        "  print(''.join(out))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEwBgRlai6bo",
        "outputId": "7ce2de55-9f9b-4128-ccd4-f9519100f8c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "junide.\n",
            "janasah.\n",
            "p.\n",
            "cony.\n",
            "a.\n",
            "nn.\n",
            "kohin.\n",
            "tolian.\n",
            "juee.\n",
            "ksahnaauranilevias.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_likelihood = 0.0\n",
        "n = 0\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2 in zip(chs, chs[1:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    prob = P[ix1][ix2]\n",
        "    logprob = torch.log(prob)\n",
        "    log_likelihood += logprob\n",
        "    n += 1\n",
        "\n",
        "print(f'{log_likelihood=}')\n",
        "nll = -log_likelihood\n",
        "print(f'{nll=}')\n",
        "print(f'{nll/n}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swoAVSJEmPKB",
        "outputId": "ef374e87-e960-4d24-b801-77d6934a29b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log_likelihood=tensor(-559951.5625)\n",
            "nll=tensor(559951.5625)\n",
            "2.4543561935424805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xs, ys = [], []\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2 in zip(chs, chs[1:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    xs.append(ix1)\n",
        "    ys.append(ix2)\n",
        "\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)\n",
        "\n",
        "print(xs[:5])\n",
        "print(ys[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4r2J0LDpFoR",
        "outputId": "c0ee94f8-2690-4b50-99fc-473df17b75ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0,  5, 13, 13,  1])\n",
            "tensor([ 5, 13, 13,  1,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "xenc = F.one_hot(xs, num_classes=27).float()\n",
        "xenc.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrq1AboHpiy3",
        "outputId": "1bdc654c-96a8-485c-fc9c-e5c9c04c14a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([228146, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 27 neuron each taking 27-D input\n",
        "W = torch.randn((27, 27))\n",
        "logits = xenc @ W   # 228146, 27 => Each row shows up amount of firing for all 27 character for a given input character\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(1, keepdims=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "LEDsDTYn2PUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(xenc, ys, epochs, lr, W=None):\n",
        "  if W is None:\n",
        "    W = torch.randn((27, 27), requires_grad=True)\n",
        "  else:\n",
        "    W.requires_grad_(True)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    # Set learning rate based on epoch\n",
        "    lr = 1 if epoch < 600 else 0.5\n",
        "\n",
        "    # forward pass\n",
        "    logits = xenc @ W\n",
        "    counts = logits.exp()\n",
        "    probs = counts / counts.sum(1, keepdims=True)\n",
        "    loss = -probs[torch.arange(len(ys)), ys].log().mean()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "      print(f\"Epoch: {epoch}, Loss: {loss.item():.4f}, LR: {lr}\")\n",
        "\n",
        "    # backward pass\n",
        "    W.grad = None\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    W.data += -lr * W.grad\n",
        "\n",
        "  return W\n",
        "\n",
        "\n",
        "model = train(xenc, ys, epochs=1000, lr=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlU-rRpL4EM3",
        "outputId": "3f570102-6289-4f31-f2a3-1e600a884410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 3.8686, LR: 1\n",
            "Epoch: 10, Loss: 3.7535, LR: 1\n",
            "Epoch: 20, Loss: 3.6530, LR: 1\n",
            "Epoch: 30, Loss: 3.5644, LR: 1\n",
            "Epoch: 40, Loss: 3.4860, LR: 1\n",
            "Epoch: 50, Loss: 3.4164, LR: 1\n",
            "Epoch: 60, Loss: 3.3545, LR: 1\n",
            "Epoch: 70, Loss: 3.2993, LR: 1\n",
            "Epoch: 80, Loss: 3.2499, LR: 1\n",
            "Epoch: 90, Loss: 3.2055, LR: 1\n",
            "Epoch: 100, Loss: 3.1654, LR: 1\n",
            "Epoch: 110, Loss: 3.1291, LR: 1\n",
            "Epoch: 120, Loss: 3.0959, LR: 1\n",
            "Epoch: 130, Loss: 3.0655, LR: 1\n",
            "Epoch: 140, Loss: 3.0376, LR: 1\n",
            "Epoch: 150, Loss: 3.0118, LR: 1\n",
            "Epoch: 160, Loss: 2.9879, LR: 1\n",
            "Epoch: 170, Loss: 2.9658, LR: 1\n",
            "Epoch: 180, Loss: 2.9452, LR: 1\n",
            "Epoch: 190, Loss: 2.9260, LR: 1\n",
            "Epoch: 200, Loss: 2.9081, LR: 1\n",
            "Epoch: 210, Loss: 2.8913, LR: 1\n",
            "Epoch: 220, Loss: 2.8756, LR: 1\n",
            "Epoch: 230, Loss: 2.8609, LR: 1\n",
            "Epoch: 240, Loss: 2.8471, LR: 1\n",
            "Epoch: 250, Loss: 2.8341, LR: 1\n",
            "Epoch: 260, Loss: 2.8219, LR: 1\n",
            "Epoch: 270, Loss: 2.8104, LR: 1\n",
            "Epoch: 280, Loss: 2.7996, LR: 1\n",
            "Epoch: 290, Loss: 2.7893, LR: 1\n",
            "Epoch: 300, Loss: 2.7797, LR: 1\n",
            "Epoch: 310, Loss: 2.7705, LR: 1\n",
            "Epoch: 320, Loss: 2.7619, LR: 1\n",
            "Epoch: 330, Loss: 2.7537, LR: 1\n",
            "Epoch: 340, Loss: 2.7459, LR: 1\n",
            "Epoch: 350, Loss: 2.7385, LR: 1\n",
            "Epoch: 360, Loss: 2.7315, LR: 1\n",
            "Epoch: 370, Loss: 2.7248, LR: 1\n",
            "Epoch: 380, Loss: 2.7184, LR: 1\n",
            "Epoch: 390, Loss: 2.7124, LR: 1\n",
            "Epoch: 400, Loss: 2.7066, LR: 1\n",
            "Epoch: 410, Loss: 2.7011, LR: 1\n",
            "Epoch: 420, Loss: 2.6958, LR: 1\n",
            "Epoch: 430, Loss: 2.6907, LR: 1\n",
            "Epoch: 440, Loss: 2.6859, LR: 1\n",
            "Epoch: 450, Loss: 2.6813, LR: 1\n",
            "Epoch: 460, Loss: 2.6768, LR: 1\n",
            "Epoch: 470, Loss: 2.6726, LR: 1\n",
            "Epoch: 480, Loss: 2.6685, LR: 1\n",
            "Epoch: 490, Loss: 2.6646, LR: 1\n",
            "Epoch: 500, Loss: 2.6608, LR: 1\n",
            "Epoch: 510, Loss: 2.6571, LR: 1\n",
            "Epoch: 520, Loss: 2.6536, LR: 1\n",
            "Epoch: 530, Loss: 2.6503, LR: 1\n",
            "Epoch: 540, Loss: 2.6470, LR: 1\n",
            "Epoch: 550, Loss: 2.6438, LR: 1\n",
            "Epoch: 560, Loss: 2.6408, LR: 1\n",
            "Epoch: 570, Loss: 2.6378, LR: 1\n",
            "Epoch: 580, Loss: 2.6350, LR: 1\n",
            "Epoch: 590, Loss: 2.6322, LR: 1\n",
            "Epoch: 600, Loss: 2.6295, LR: 0.5\n",
            "Epoch: 610, Loss: 2.6282, LR: 0.5\n",
            "Epoch: 620, Loss: 2.6269, LR: 0.5\n",
            "Epoch: 630, Loss: 2.6257, LR: 0.5\n",
            "Epoch: 640, Loss: 2.6244, LR: 0.5\n",
            "Epoch: 650, Loss: 2.6232, LR: 0.5\n",
            "Epoch: 660, Loss: 2.6220, LR: 0.5\n",
            "Epoch: 670, Loss: 2.6208, LR: 0.5\n",
            "Epoch: 680, Loss: 2.6196, LR: 0.5\n",
            "Epoch: 690, Loss: 2.6184, LR: 0.5\n",
            "Epoch: 700, Loss: 2.6173, LR: 0.5\n",
            "Epoch: 710, Loss: 2.6162, LR: 0.5\n",
            "Epoch: 720, Loss: 2.6151, LR: 0.5\n",
            "Epoch: 730, Loss: 2.6140, LR: 0.5\n",
            "Epoch: 740, Loss: 2.6129, LR: 0.5\n",
            "Epoch: 750, Loss: 2.6118, LR: 0.5\n",
            "Epoch: 760, Loss: 2.6108, LR: 0.5\n",
            "Epoch: 770, Loss: 2.6097, LR: 0.5\n",
            "Epoch: 780, Loss: 2.6087, LR: 0.5\n",
            "Epoch: 790, Loss: 2.6077, LR: 0.5\n",
            "Epoch: 800, Loss: 2.6067, LR: 0.5\n",
            "Epoch: 810, Loss: 2.6057, LR: 0.5\n",
            "Epoch: 820, Loss: 2.6047, LR: 0.5\n",
            "Epoch: 830, Loss: 2.6038, LR: 0.5\n",
            "Epoch: 840, Loss: 2.6028, LR: 0.5\n",
            "Epoch: 850, Loss: 2.6019, LR: 0.5\n",
            "Epoch: 860, Loss: 2.6009, LR: 0.5\n",
            "Epoch: 870, Loss: 2.6000, LR: 0.5\n",
            "Epoch: 880, Loss: 2.5991, LR: 0.5\n",
            "Epoch: 890, Loss: 2.5982, LR: 0.5\n",
            "Epoch: 900, Loss: 2.5974, LR: 0.5\n",
            "Epoch: 910, Loss: 2.5965, LR: 0.5\n",
            "Epoch: 920, Loss: 2.5956, LR: 0.5\n",
            "Epoch: 930, Loss: 2.5948, LR: 0.5\n",
            "Epoch: 940, Loss: 2.5939, LR: 0.5\n",
            "Epoch: 950, Loss: 2.5931, LR: 0.5\n",
            "Epoch: 960, Loss: 2.5923, LR: 0.5\n",
            "Epoch: 970, Loss: 2.5915, LR: 0.5\n",
            "Epoch: 980, Loss: 2.5907, LR: 0.5\n",
            "Epoch: 990, Loss: 2.5899, LR: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_name(count):\n",
        "  for i in range(count):\n",
        "    out = []\n",
        "    ix = 0\n",
        "    while True:\n",
        "      xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
        "      logits = xenc @ model\n",
        "      counts = logits.exp()\n",
        "      p = counts/ counts.sum(1, keepdims=True)\n",
        "      ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
        "      out.append(itos[ix])\n",
        "      if ix == 0:\n",
        "        break\n",
        "    print(''.join(out))\n",
        "\n",
        "generate_name(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-BwEe876BhL",
        "outputId": "fea7a13c-6902-4d5c-fb6f-bf79d5a49010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jaue.\n",
            "a.\n",
            "kwpejbionzqppwarbihisisajbrakamorvxqkadeela.\n",
            "az.\n",
            "arileri.\n",
            "chaiadayra.\n",
            "fbrlqdo.\n",
            "meyjon.\n",
            "zqzrabran.\n",
            "han.\n"
          ]
        }
      ]
    }
  ]
}